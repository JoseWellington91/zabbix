##CentOS 7.3  (3.10.0-514.26.2.el7.x86_64)

# 3 servers
# 10.10.8.210(k8s-master)
# 10.10.8.211(k8s-node-1)
# 10.10.8.212(k8s-node-2)


## echo 0 > /proc/sys/net/ipv4/ip_forward

################ INSTALL CFSSL (ON All Server) ############### 
##PKI工具集 (public key infrastructure，缩写为PKI)

cd /usr/sbin
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O cfssl
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O cfssljson
wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O cfssl-certinfo
chmod +x cfssl*


#生成证书(任意一台机器生成，之后发布到所有机器)

mkdir /root/ssl && cd /root/ssl
cfssl print-defaults config > config.json
cfssl print-defaults csr > csr.json

cat  > ca-config.json <<EOF
{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ],
        "expiry": "8760h"
      }
    }
  }
}
EOF

#ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；
#signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；
#server auth：表示 client 可以用该 CA 对 server 提供的证书进行验证；
#client auth：表示 server 可以用该 CA 对 client 提供的证书进行验证；


cat > ca-csr.json << EOF
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF

#"CN"：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；
#"O"：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；


cfssl gencert -initca ca-csr.json | cfssljson -bare ca

#分发证书，将证书拷贝到所有机器上
#ON ALL SERVERS,DO THIS:
mkdir -p /etc/kubernetes/ssl
cp ca* /etc/kubernetes/ssl


######  定义全局变量(ON ALL SERVERS) ##########

cat > /root/environment.sh << EOF
#!/usr/bin/bash                                                                                                                                                          

# TLS Bootstrapping 使用的 Token，可以使用命令 head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 生成
BOOTSTRAP_TOKEN="41f7e4ba8b7be874fcff18bf5cf41a7c"

# 最好使用 主机未用的网段 来定义服务网段和 Pod 网段

# 服务网段 (Service CIDR），部署前路由不可达，部署后集群内使用IP:Port可达
SERVICE_CIDR="10.254.0.0/16"
    
# POD 网段 (Cluster CIDR），部署前路由不可达，**部署后**路由可达(flanneld保)
CLUSTER_CIDR="172.30.0.0/16"
    
# 服务端口范围 (NodePort Range)
export NODE_PORT_RANGE="8400-9000"
    
# etcd 集群服务地址列表
export ETCD_ENDPOINTS="https://10.10.8.210:2379,https://10.10.8.211:2379,https://10.10.8.212:2379"
    
# flanneld 网络配置前缀
export FLANNEL_ETCD_PREFIX="/feezu/network"
    
# kubernetes 服务 IP (一般是 SERVICE_CIDR 中第一个IP)
export CLUSTER_KUBERNETES_SVC_IP="10.254.0.1"
    
# 集群 DNS 服务 IP (从 SERVICE_CIDR 中预分)
export CLUSTER_DNS_SVC_IP="10.254.0.2"
    
# 集群 DNS 域名
export CLUSTER_DNS_DOMAIN="cluster.local."

EOF


###############INSTALL ETCD CLUSTER (ON All SERVERS) ################
cat > /etc/hosts <<EOF
10.10.8.210 k8s-master
10.10.8.211 k8s-node-1
10.10.8.212 k8s-node-2
EOF

# 当前部署的机器名称
export NODE_NAME=k8s-master

# 当前部署的机器 IP
export NODE_IP=10.10.8.210 

# etcd 集群所有机器 IP
export NODE_IPS="10.10.8.210 10.10.8.211 10.10.8.212" 

# etcd 集群间通信的IP和端口
export ETCD_NODES=k8s-master=https://10.10.8.210:2380,k8s-node-1=https://10.10.8.211:2380,k8s-node-2=https://10.10.8.212:2380

# 导入用到的其它全局变量：ETCD_ENDPOINTS、FLANNEL_ETCD_PREFIX、CLUSTER_CIDR
source /root/environment.sh


cd /Data/app
wget https://github.com/coreos/etcd/releases/download/v3.1.6/etcd-v3.1.6-linux-amd64.tar.gz
tar zxvf etcd-v3.1.6-linux-amd64.tar.gz
mv etcd-v3.1.6-linux-amd64/etcd*  /usr/sbin



#创建etcd证书签名请求
cat > etcd-csr.json << EOF
{
  "CN": "etcd",
  "hosts": [
    "127.0.0.1",
    "${NODE_IP}"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF
# hosts 字段指定授权使用该证书的 etcd 节点 IP；


#生成etcd证书和私钥
cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
  -ca-key=/etc/kubernetes/ssl/ca-key.pem \
  -config=/etc/kubernetes/ssl/ca-config.json \
  -profile=kubernetes etcd-csr.json | cfssljson -bare etcd


mkdir -p /etc/etcd/ssl
mv etcd*.pem /etc/etcd/ssl
rm -f etcd.csr etcd-csr.json



#创建etcd的systemd unit文件
#创建工作目录
mkdir /var/lib/etcd

cat > /usr/lib/systemd/system/etcd.service << EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/usr/sbin/etcd \\
  --name=${NODE_NAME} \\
  --cert-file=/etc/etcd/ssl/etcd.pem \\
  --key-file=/etc/etcd/ssl/etcd-key.pem \\
  --peer-cert-file=/etc/etcd/ssl/etcd.pem \\
  --peer-key-file=/etc/etcd/ssl/etcd-key.pem \\
  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --initial-advertise-peer-urls=https://${NODE_IP}:2380 \\
  --listen-peer-urls=https://${NODE_IP}:2380 \\
  --listen-client-urls=https://${NODE_IP}:2379,http://127.0.0.1:2379 \\
  --advertise-client-urls=https://${NODE_IP}:2379 \\
  --initial-cluster-token=etcd-cluster-0 \\
  --initial-cluster=${ETCD_NODES} \\
  --initial-cluster-state=new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

#指定 etcd 的工作目录和数据目录为 /var/lib/etcd，需在启动服务前创建这个目录；
#为了保证通信安全，需要指定 etcd 的公私钥(cert-file和key-file)、Peers 通信的公私钥和 CA 证书(peer-cert-file、peer-key-file、peer-trusted-ca-file)、客户端的CA证书（trusted-ca-file）；
#--initial-cluster-state 值为 new 时，--name 的参数值必须位于 --initial-cluster 列表中；

#启动etcd
systemctl daemon-reload
systemctl enable etcd
systemctl start etcd
systemctl status etcd


# INSTALL ETCD ON OTHER SERVETS#

# 验证etcd集群的运行状态
# 部署完 etcd 集群后，在任一 etcd 集群节点上执行如下命令：
for ip in ${NODE_IPS};do
  ETCDCTL_API=3 /usr/sbin/etcdctl \
  --endpoints=https://${ip}:2379  \
  --cacert=/etc/kubernetes/ssl/ca.pem \
  --cert=/etc/etcd/ssl/etcd.pem \
  --key=/etc/etcd/ssl/etcd-key.pem \
  endpoint health;
done

#三台 etcd 的输出均为 healthy 时表示集群服务正常（忽略 warning 信息）
#2017-08-03 23:56:07.495929 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated
#https://10.10.8.210:2379 is healthy: successfully committed proposal: took = 2.099047ms
#2017-08-03 23:56:07.535874 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated
#https://10.10.8.211:2379 is healthy: successfully committed proposal: took = 1.920639ms
#2017-08-03 23:56:07.574565 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated
#https://10.10.8.212:2379 is healthy: successfully committed proposal: took = 1.999241ms




#############INSTALL kubectl (ON ALL SERVERS)###############

#kubectl 默认从 ~/.kube/config 配置文件获取访问 kube-apiserver 地址、证书、用户名等信息，如果没有配置该文件，执行命令时出错：

#定义变量
export MASTER_IP=10.10.8.210
export KUBE_APISERVER="https://${MASTER_IP}:6443"


# download kubectl
cd /Data/app
wget https://dl.k8s.io/v1.6.2/kubernetes-server-linux-amd64.tar.gz
tar zxvf  kubernetes-server-linux-amd64.tar.gz
#wget https://dl.k8s.io/v1.6.2/kubernetes-client-linux-amd64.tar.gz
#tar zxvf kubernetes-client-linux-amd64.tar.gz
cp kubernetes/server/bin/kubectl /usr/sbin


#创建admin证书
# kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥
# 创建 admin 证书签名请求
cat > admin-csr.json << EOF
{
  "CN": "admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}
EOF

#后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权；
#kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 所有 API的权限；
#O 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver 时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限；
#hosts 属性值为空列表；

#生成 admin 证书和私钥：

cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
  -ca-key=/etc/kubernetes/ssl/ca-key.pem \
  -config=/etc/kubernetes/ssl/ca-config.json \
  -profile=kubernetes admin-csr.json | cfssljson -bare admin

mv admin*.pem /etc/kubernetes/ssl/
rm -f admin.csr admin-csr.json

# 创建kubectl kubeconfig 文件
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER}
# 设置客户端认证参数
kubectl config set-credentials admin \
  --client-certificate=/etc/kubernetes/ssl/admin.pem \
  --embed-certs=true \
  --client-key=/etc/kubernetes/ssl/admin-key.pem
# 设置上下文参数
kubectl config set-context kubernetes \
  --cluster=kubernetes \
  --user=admin
# 设置默认上下文
kubectl config use-context kubernetes

#admin.pem 证书 O 字段值为 system:masters，kube-apiserver 预定义的 RoleBinding cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 相关 API 的权限；
#生成的 kubeconfig 被保存到 ~/.kube/config 文件；



############INSTALL Flannel网络(ON ALL SERVERS)#############
# kubernetes 要求集群内各节点能通过 Pod 网段互联互通
#定义变量
export NODE_IP=10.10.8.210
#当前节点的IP

source /root/environment.sh
#导入其他全局变量 ETCD_ENDPOINTS、FLANNEL_ETCD_PREFIX、CLUSTER_CIDR


#创建TLS密钥和证书。etcd集群启用了双向TLS认证,所以需要为flanneld指定与etcd集群通信的CA和秘钥
#创建 flanneld 证书签名请求：
cat > flanneld-csr.json <<EOF
{
  "CN": "flanneld",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF
#hosts 字段为空；

#生成 flanneld 证书和私钥
cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
  -ca-key=/etc/kubernetes/ssl/ca-key.pem \
  -config=/etc/kubernetes/ssl/ca-config.json \
  -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld

mkdir -p /etc/flanneld/ssl
mv flanneld*.pem /etc/flanneld/ssl
rm -f flanneld.csr  flanneld-csr.json



# 向 etcd 写入集群 Pod 网段信息#
# 注意：本步骤只需在第一次部署 Flannel 网络时执行，后续在其它节点上部署 Flannel 时无需再写入该信息！
etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/etc/kubernetes/ssl/ca.pem \
  --cert-file=/etc/flanneld/ssl/flanneld.pem \
  --key-file=/etc/flanneld/ssl/flanneld-key.pem \
  set ${FLANNEL_ETCD_PREFIX}/config '{"Network":"'${CLUSTER_CIDR}'", "SubnetLen": 24, "Backend": {"Type": "vxlan"}}'

#flanneld 目前版本 (v0.7.1) 不支持 etcd v3，故使用 etcd v2 API 写入配置 key 和网段数据；
#写入的 Pod 网段(${CLUSTER_CIDR}，172.30.0.0/16) 必须与 kube-controller-manager 的 --cluster-cidr 选项值一致；


#安装和配置Flanneld

#download flannel
cd /Data/app
wget https://github.com/coreos/flannel/releases/download/v0.7.1/flannel-v0.7.1-linux-amd64.tar.gz
tar zxvf flannel-v0.7.1-linux-amd64.tar.gz
cp flanneld mk-docker-opts.sh /usr/sbin


#创建 flanneld 的 systemd unit 文件

cat > /usr/lib/systemd/system/flanneld.service << EOF
[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
ExecStart=/usr/sbin/flanneld \\
  -etcd-cafile=/etc/kubernetes/ssl/ca.pem \\
  -etcd-certfile=/etc/flanneld/ssl/flanneld.pem \\
  -etcd-keyfile=/etc/flanneld/ssl/flanneld-key.pem \\
  -etcd-endpoints=${ETCD_ENDPOINTS} \\
  -etcd-prefix=${FLANNEL_ETCD_PREFIX}
ExecStartPost=/usr/sbin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=on-failure

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
EOF

#mk-docker-opts.sh 脚本将分配给 flanneld 的 Pod 子网网段信息写入到 /run/flannel/docker 文件中，后续 docker 启动时使用这个文件中参数值设置 docker0 网桥；
#flanneld 使用系统缺省路由所在的接口和其它节点通信，对于有多个网络接口的机器（如，内网和公网），可以用 -iface 选项值指定通信接口(上面的 systemd unit 文件没指定这个选项)；

#启动flanneld
systemctl daemon-reload
systemctl enable flanneld
systemctl start flanneld
systemctl status flanneld


#检查 flanneld 服务
journalctl  -u flanneld |grep 'Lease acquired'
ip addr show flannel.1


#检查分配给各 flanneld 的 Pod 网段信息
 # 查看集群 Pod 网段(/16)
etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/etc/kubernetes/ssl/ca.pem \
  --cert-file=/etc/flanneld/ssl/flanneld.pem \
  --key-file=/etc/flanneld/ssl/flanneld-key.pem \
  get ${FLANNEL_ETCD_PREFIX}/config
#输出如下:
{"Network": "172.30.0.0/16", "SubnetLen": 24, "Backend": { "Type": "vxlan" }}


# 查看已分配的 Pod 子网段列表(/24)
etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/etc/kubernetes/ssl/ca.pem \
  --cert-file=/etc/flanneld/ssl/flanneld.pem \
  --key-file=/etc/flanneld/ssl/flanneld-key.pem \
  ls ${FLANNEL_ETCD_PREFIX}/subnets

#输出如下(具体网段可能与示例不同):
/feezu/network/subnets/172.30.40.0-24
/feezu/network/subnets/172.30.56.0-24
/feezu/network/subnets/172.30.28.0-24

# 查看某一 Pod 网段对应的 flanneld 进程监听的 IP 和网络参数
etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/etc/kubernetes/ssl/ca.pem \
  --cert-file=/etc/flanneld/ssl/flanneld.pem \
  --key-file=/etc/flanneld/ssl/flanneld-key.pem \
  get ${FLANNEL_ETCD_PREFIX}/subnets/172.30.56.0-24

#输出如下：
{"PublicIP":"10.10.8.210","BackendType":"vxlan","BackendData":{"VtepMAC":"b6:62:58:65:97:ed"}}


#确保各节点间 Pod 网段能互联互通

#在各节点上部署完 Flannel 后，查看已分配的 Pod 子网段列表(/24)(ON any server)

etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/etc/kubernetes/ssl/ca.pem \
  --cert-file=/etc/flanneld/ssl/flanneld.pem \
  --key-file=/etc/flanneld/ssl/flanneld-key.pem \
  ls ${FLANNEL_ETCD_PREFIX}/subnets

#输出如下：
/feezu/network/subnets/172.30.83.0-24
/feezu/network/subnets/172.30.66.0-24
/feezu/network/subnets/172.30.67.0-24

#当前三个节点分配的 Pod 网段分别是：172.30.83.0-24、172.30.66.0-24、172.30.67.0-24。



#在各节点上分配 ping 这三个网段的网关地址，确保能通：
ping 172.30.83.0
ping 172.30.66.0
ping 172.30.67.0





########################部署master节点##########################
#kube-apiserver
#kube-scheduler
#kube-controller-manager

#定义变量
export MASTER_IP=10.10.8.210

#导入其他变量
source /root/environment.sh


#下载最新版本的二进制文件
cd /Data/software
wget https://dl.k8s.io/v1.6.2/kubernetes-server-linux-amd64.tar.gz

tar zxvf  kubernetes-server-linux-amd64.tar.gz
cd kubernetes
cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler}  /usr/sbin/


#安装和配置flanneld(之前步骤已完成)


#创建 kubernetes 证书
#创建 kubernetes 证书签名请求

cat > kubernetes-csr.json <<EOF
{
  "CN": "kubernetes",
  "hosts": [
    "127.0.0.1",
    "${MASTER_IP}",
    "${CLUSTER_KUBERNETES_SVC_IP}",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF

#如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，所以上面分别指定了当前部署的 master 节点主机 IP；

#还需要添加 kube-apiserver 注册的名为 kubernetes 的服务 IP (Service Cluster IP)，一般是 kube-apiserver --service-cluster-ip-range 选项值指定的网段的第一个IP，如 "10.254.0.1"；





#生成 kubernetes 证书和私钥

cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
  -ca-key=/etc/kubernetes/ssl/ca-key.pem \
  -config=/etc/kubernetes/ssl/ca-config.json \
  -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes

mkdir -p /etc/kubernetes/ssl/
mv kubernetes*.pem /etc/kubernetes/ssl/
rm -f kubernetes.csr  kubernetes-csr.json



#配置和启动 kube-apiserver
#kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token.csv 一致，如果一致则自动为 kubelet生成证书和秘钥。

# 导入的 environment.sh 文件定义了 BOOTSTRAP_TOKEN 变量
cat > token.csv <<EOF
${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,"system:kubelet-bootstrap"
EOF
mv token.csv /etc/kubernetes/

#创建 kube-apiserver 的 systemd unit 文件

cat  > /usr/lib/systemd/system/kube-apiserver.service <<EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
ExecStart=/usr/sbin/kube-apiserver \\
  --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --advertise-address=${MASTER_IP} \\
  --bind-address=${MASTER_IP} \\
  --insecure-bind-address=${MASTER_IP} \\
  --authorization-mode=RBAC \\
  --runtime-config=rbac.authorization.k8s.io/v1alpha1 \\
  --kubelet-https=true \\
  --experimental-bootstrap-token-auth \\
  --token-auth-file=/etc/kubernetes/token.csv \\
  --service-cluster-ip-range=${SERVICE_CIDR} \\
  --service-node-port-range=${NODE_PORT_RANGE} \\
  --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\
  --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\
  --client-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\
  --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\
  --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \\
  --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \\
  --etcd-servers=${ETCD_ENDPOINTS} \\
  --enable-swagger-ui=true \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/lib/audit.log \\
  --event-ttl=1h \\
  --v=2
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

#kube-apiserver 1.6 版本开始使用 etcd v3 API 和存储格式；
#--authorization-mode=RBAC 指定在安全端口使用 RBAC 授权模式，拒绝未通过授权的请求；
#kube-scheduler、kube-controller-manager 一般和 kube-apiserver 部署在同一台机器上，它们使用非安全端口和 kube-apiserver通信;
#kubelet、kube-proxy、kubectl 部署在其它 Node 节点上，如果通过安全端口访问 kube-apiserver，则必须先通过 TLS 证书认证，再通过 RBAC 授权；
#kube-proxy、kubectl 通过在使用的证书里指定相关的 User、Group 来达到通过 RBAC 授权的目的；
#如果使用了 kubelet TLS Boostrap 机制，则不能再指定 --kubelet-certificate-authority、--kubelet-client-certificate 和 --kubelet-client-key 选项，否则后续 kube-apiserver 校验 kubelet 证书时出现 ”x509: certificate signed by unknown authority“ 错误；
#--admission-control 值必须包含 ServiceAccount，否则部署集群插件时会失败；
#--bind-address 不能为 127.0.0.1；
#--service-cluster-ip-range 指定 Service Cluster IP 地址段，该地址段不能路由可达；
#--service-node-port-range=${NODE_PORT_RANGE} 指定 NodePort 的端口范围；
#缺省情况下 kubernetes 对象保存在 etcd /registry 路径下，可以通过 --etcd-prefix 参数进行调整；




#配置和启动 kube-controller-manager
#创建 kube-controller-manager 的 systemd unit 文件

cat > /usr/lib/systemd/system/kube-controller-manager.service <<EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/sbin/kube-controller-manager \\
  --address=127.0.0.1 \\
  --master=http://${MASTER_IP}:8080 \\
  --allocate-node-cidrs=true \\
  --service-cluster-ip-range=${SERVICE_CIDR} \\
  --cluster-cidr=${CLUSTER_CIDR} \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\
  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\
  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\
  --root-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --leader-elect=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

#--address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器，否则：
kubectl get componentstatuses
NAME                 STATUS      MESSAGE                                                                                        ERROR
controller-manager   Unhealthy   Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: getsockopt: connection refused
scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: getsockopt: connection refused

#--master=http://{MASTER_IP}:8080：使用非安全 8080 端口与 kube-apiserver 通信；
#--cluster-cidr 指定 Cluster 中 Pod 的 CIDR 范围，该网段在各 Node 间必须路由可达(flanneld保证)；
#--service-cluster-ip-range 参数指定 Cluster 中 Service 的CIDR范围，该网络在各 Node 间必须路由不可达，必须和 kube-apiserver 中的参数一致；
#--cluster-signing-* 指定的证书和私钥文件用来签名为 TLS BootStrap 创建的证书和私钥；
#--root-ca-file 用来对 kube-apiserver 证书进行校验，指定该参数后，才会在Pod 容器的 ServiceAccount 中放置该 CA 证书文件；
#--leader-elect=true 部署多台机器组成的 master 集群时选举产生一处于工作状态的 kube-controller-manager 进程；





#配置和启动 kube-scheduler
#创建 kube-scheduler 的 systemd unit 文件
cat > /usr/lib/systemd/system/kube-scheduler.service <<EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/sbin/kube-scheduler \\
  --address=127.0.0.1 \\
  --master=http://${MASTER_IP}:8080 \\
  --leader-elect=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

#--address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器；
#--master=http://{MASTER_IP}:8080：使用非安全 8080 端口与 kube-apiserver 通信；
#--leader-elect=true 部署多台机器组成的 master 集群时选举产生一处于工作状态的 kube-controller-manager 进程；


systemctl daemon-reload
systemctl enable kube-apiserver kube-controller-manager kube-scheduler

#启动服务
systemctl start kube-apiserver
systemctl start kube-controller-manager
systemctl start kube-scheduler


#验证 master 节点功能
kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {"health": "true"}
etcd-1               Healthy   {"health": "true"}
etcd-2               Healthy   {"health": "true"}





##########################部署NODE节点############################

#flanneld
#docker
#kubelet
#kube-proxy

#定义和引入变量
export MASTER_IP=10.10.8.210
export KUBE_APISERVER="https://${MASTER_IP}:6443"
export NODE_IP=10.10.8.211
source /root/environment.sh

##安装和配置 flanneld,之前已完成


#安装和配置 docker
cd /Data/software
wget https://get.docker.com/builds/Linux/x86_64/docker-17.04.0-ce.tgz
tar zxvf docker-17.04.0-ce.tgz
cp docker/docker* /usr/sbin/
cp docker/completion/bash/docker /etc/bash_completion.d/


#创建 docker 的 systemd unit 文件

cat > /usr/lib/systemd/system/docker.service << EOF
[Unit]
Description=Docker Application Container Engine
Documentation=http://docs.docker.io

[Service]
Environment="PATH=/bin:/sbin:/usr/bin:/usr/sbin"
EnvironmentFile=-/run/flannel/docker
ExecStart=/usr/sbin/dockerd --log-level=error \$DOCKER_NETWORK_OPTIONS
ExecReload=/bin/kill -s HUP \$MAINPID
Restart=on-failure
RestartSec=5                                                                                                                                                             
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
Delegate=yes
KillMode=process

[Install]
WantedBy=multi-user.target
EOF

#dockerd 运行时会调用其它 docker 命令，如 docker-proxy，所以需要将 docker 命令所在的目录加到 PATH 环境变量中；
#flanneld 启动时将网络配置写入到 /run/flannel/docker 文件中的变量 DOCKER_NETWORK_OPTIONS，dockerd 命令行上指定该变量值来设置 docker0 网桥参数；
#如果指定了多个 EnvironmentFile 选项，则必须将 /run/flannel/docker 放在最后(确保 docker0 使用 flanneld 生成的 bip 参数)；
#不能关闭默认开启的 --iptables 和 --ip-masq 选项；
#如果内核版本比较新，建议使用 overlay 存储驱动；
#docker 从 1.13 版本开始，可能将 iptables FORWARD chain的默认策略设置为DROP，从而导致 ping 其它 Node 上的 Pod IP 失败，遇到这种情况时，需要手动设置策略为 ACCEPT：
iptables -P FORWARD ACCEPT
#并且把以下命令写入/etc/rc.local文件中，防止节点重启iptables FORWARD chain的默认策略又还原为DROP
sleep 60 && /sbin/iptables -P FORWARD ACCEPT
#为了加快 pull image 的速度，可以使用国内的仓库镜像服务器，同时增加下载的并发数。(如果 dockerd 已经运行，则需要重启 dockerd 生效。)
cat /etc/docker/daemon.json
{
  "registry-mirrors": ["https://docker.mirrors.ustc.edu.cn", "hub-mirror.c.163.com"],
  "max-concurrent-downloads": 10
}


#启动docker
systemctl daemon-reload
systemctl enable docker
systemctl start docker


#安装和配置 kubelet
#kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求(certificatesigningrequests)：

#在master上执行(只需执行一次)：
kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap
#--user=kubelet-bootstrap 是文件 /etc/kubernetes/token.csv 中指定的用户名，同时也写入了文件 /etc/kubernetes/bootstrap.kubeconfig

#下载最新的 kubelet 和 kube-proxy 二进制文件
wget https://dl.k8s.io/v1.6.2/kubernetes-server-linux-amd64.tar.gz
tar zxvf kubernetes-server-linux-amd64.tar.gz
cd kubernetes
tar zxvf  kubernetes-src.tar.gz
cp -r ./server/bin/{kube-proxy,kubelet} /usr/sbin/


#创建 kubelet bootstrapping kubeconfig 文件(在node上执行)
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=bootstrap.kubeconfig
# 设置客户端认证参数
kubectl config set-credentials kubelet-bootstrap \
  --token=${BOOTSTRAP_TOKEN} \
  --kubeconfig=bootstrap.kubeconfig
# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kubelet-bootstrap \
  --kubeconfig=bootstrap.kubeconfig
# 设置默认上下文
kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
mv bootstrap.kubeconfig /etc/kubernetes/
#--embed-certs 为 true 时表示将 certificate-authority 证书写入到生成的 bootstrap.kubeconfig 文件中；
设置 kubelet 客户端认证参数时没有指定秘钥和证书，后续由 kube-apiserver 自动生成；


#创建 kubelet 的 systemd unit 文件

#创建工作目录
mkdir /var/lib/kubelet

cat > /usr/lib/systemd/system/kubelet.service << EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/usr/sbin/kubelet \\
  --address=${NODE_IP} \\
  --hostname-override=${NODE_IP} \\
  --pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest \\
  --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\
  --require-kubeconfig \\
  --cert-dir=/etc/kubernetes/ssl \\
  --cluster-dns=${CLUSTER_DNS_SVC_IP} \\
  --cluster-domain=${CLUSTER_DNS_DOMAIN} \\
  --hairpin-mode promiscuous-bridge \\
  --allow-privileged=true \\
  --serialize-image-pulls=false \\
  --logtostderr=true \\
  --v=2
ExecStartPost=/sbin/iptables -A INPUT -s 10.0.0.0/8 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -s 172.16.0.0/12 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -s 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROP
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

#--address 不能设置为 127.0.0.1，否则后续 Pods 访问 kubelet 的 API 接口时会失败，因为 Pods 访问的 127.0.0.1 指向自己而不是 kubelet；
#如果设置了 --hostname-override 选项，则 kube-proxy 也需要设置该选项，否则会出现找不到 Node 的情况；
#--experimental-bootstrap-kubeconfig 指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；
#管理员通过了 CSR 请求后，kubelet 自动在 --cert-dir 目录创建证书和私钥文件(kubelet-client.crt 和 kubelet-client.key)，然后写入 --kubeconfig 文件(自动创建 --kubeconfig 指定的文件)；
#建议在 --kubeconfig 配置文件中指定 kube-apiserver 地址，如果未指定 --api-servers 选项，则必须指定 --require-kubeconfig 选项后才从配置文件中读取 kue-apiserver 的地址，否则 kubelet 启动后将找不到 kube-apiserver (日志中提示未找到 API Server），kubectl get nodes 不会返回对应的 Node 信息;
#--cluster-dns 指定 kubedns 的 Service IP(可以先分配，后续创建 kubedns 服务时指定该 IP)，--cluster-domain 指定域名后缀，这两个参数同时指定后才会生效；
#kubelet cAdvisor 默认在所有接口监听 4194 端口的请求，对于有外网的机器来说不安全，ExecStartPost 选项指定的 iptables 规则只允许内网机器访问 4194 端口；


#启动kubelet
systemctl daemon-reload
systemctl enable kubelet
systemctl start kubelet
systemctl status kubelet


#通过 kubelet 的 TLS 证书请求

#kubelet 首次启动时向 kube-apiserver 发送证书签名请求，必须通过后 kubernetes 系统才会将该 Node 加入到集群。

#查看未授权的 CSR 请求(在master上执行)：

kubectl get csr
#输出内容:
NAME        AGE       REQUESTOR           CONDITION
csr-2b308   4m        kubelet-bootstrap   Pending

kubectl get nodes
#输出内容:
No resources found.

#通过 CSR 请求(在master上执行,每新增一个Node执行一次)：
kubectl certificate approve csr-2b308
#输出内容：
certificatesigningrequest "csr-2b308" approved
kubectl get nodes
NAME        STATUS    AGE       VERSION
10.10.8.212   Ready     49m       v1.6.2

#自动在node端生成了 kubelet kubeconfig 文件和公私钥：
ls -l /etc/kubernetes/kubelet.kubeconfig
-rw------- 1 root root 2284 Apr  7 02:07 /etc/kubernetes/kubelet.kubeconfig

ls -l /etc/kubernetes/ssl/kubelet*
-rw-r--r-- 1 root root 1046 Apr  7 02:07 /etc/kubernetes/ssl/kubelet-client.crt
-rw------- 1 root root  227 Apr  7 02:04 /etc/kubernetes/ssl/kubelet-client.key
-rw-r--r-- 1 root root 1103 Apr  7 02:07 /etc/kubernetes/ssl/kubelet.crt
-rw------- 1 root root 1675 Apr  7 02:07 /etc/kubernetes/ssl/kubelet.key

#在其他node上重复以上步骤(部署NODE节点)



#配置 kube-proxy (ON All NODES)

#创建 kube-proxy 证书

#创建 kube-proxy 证书签名请求：

cat > kube-proxy-csr.json << EOF
{
  "CN": "system:kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF

#CN 指定该证书的 User 为 system:kube-proxy；
#kube-apiserver 预定义的 RoleBinding system:node-proxier 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；
#hosts 属性值为空列表；



#生成 kube-proxy 客户端证书和私钥：
cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
  -ca-key=/etc/kubernetes/ssl/ca-key.pem \
  -config=/etc/kubernetes/ssl/ca-config.json \
  -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy

mv kube-proxy*.pem /etc/kubernetes/ssl/
rm -f kube-proxy.csr  kube-proxy-csr.json


#创建 kube-proxy kubeconfig 文件

# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-proxy.kubeconfig
# 设置客户端认证参数
kubectl config set-credentials kube-proxy \
  --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
  --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig
# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig
# 设置默认上下文
kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
mv kube-proxy.kubeconfig /etc/kubernetes/


#设置集群参数和客户端认证参数时 --embed-certs 都为 true，这会将 certificate-authority、client-certificate 和 client-key 指向的证书文件内容写入到生成的 kube-proxy.kubeconfig 文件中；
#kube-proxy.pem 证书中 CN 为 system:kube-proxy，kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；


#创建 kube-proxy 的 systemd unit 文件

#创建工作目录
mkdir -p /var/lib/kube-proxy 

cat > /usr/lib/systemd/system/kube-proxy.service <<EOF
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/usr/sbin/kube-proxy \\
  --bind-address=${NODE_IP} \\
  --hostname-override=${NODE_IP} \\
  --cluster-cidr=${SERVICE_CIDR} \\
  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\
  --logtostderr=true \\
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

#--hostname-override 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 iptables 规则；
#--cluster-cidr 必须与 kube-apiserver 的 --service-cluster-ip-range 选项值一致；
#kube-proxy 根据 --cluster-cidr 判断集群内部和外部流量，指定 --cluster-cidr 或 --masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；
#--kubeconfig 指定的配置文件嵌入了 kube-apiserver 的地址、用户名、证书、秘钥等请求和认证信息；
#预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；


#启动kube-proxy
systemctl daemon-reload
systemctl enable kube-proxy
systemctl start kube-proxy
systemctl status kube-proxy


##验证集群功能(全部node配置完成之后,在master上进行)

#定义文件：
cd /Data/app
cat > nginx-ds.yml << EOF
apiVersion: v1
kind: Service
metadata:
  name: nginx-ds
  labels:
    app: nginx-ds
spec:
  type: NodePort
  selector:
    app: nginx-ds
  ports:
  - name: http
    port: 80
    targetPort: 80

---

apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: nginx-ds
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
      - name: my-nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
EOF

#创建 Pod 和服务：
kubectl create -f nginx-ds.yml
service "nginx-ds" created
daemonset "nginx-ds" created

#检查节点状态

kubectl get nodes
NAME        STATUS    AGE       VERSION
10.64.3.7   Ready     8d        v1.6.2
10.64.3.8   Ready     8d        v1.6.2
#都为 Ready 时正常。

#检查各 Node 上的 Pod IP 连通性

kubectl get pods  -o wide|grep nginx-ds
nginx-ds-6ktz8              1/1       Running            0          5m        172.30.25.19   10.64.3.7
nginx-ds-6ktz9              1/1       Running            0          5m        172.30.20.20   10.64.3.8
#可见，nginx-ds 的 Pod IP 分别是 172.30.25.19、172.30.20.20，在所有 Node 上分别 ping 这两个 IP，看是否连通。

#检查服务 IP 和端口可达性

kubectl get svc |grep nginx-ds
nginx-ds     10.254.136.178   <nodes>       80:8744/TCP         11m
#可见：

#服务IP：10.254.136.178
#服务端口：80
#NodePort端口：8744

#在所有 Node 上执行：
curl 10.254.136.178 # `kubectl get svc |grep nginx-ds` 输出中的服务 IP
#预期输出 nginx 欢迎页面内容。

#检查服务的 NodePort 可达性

#在所有 Node 上执行：

export NODE_IP=10.64.3.7 # 当前 Node 的 IP
export NODE_PORT=8744 # `kubectl get svc |grep nginx-ds` 输出中 80 端口映射的 NodePort
curl ${NODE_IP}:${NODE_PORT}
#预期输出 nginx 欢迎页面内容。




##################kube-dns插件(在master上)###############
#使用到的文件
#kubedns-cm.yaml  
#kubedns-controller.yaml  
#kubedns-sa.yaml  
#kubedns-svc.yaml

##查看系统预定义的 RoleBinding
kubectl get clusterrolebindings system:kube-dns -o yaml
#输出如下：
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: 2017-08-14T08:21:46Z
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-dns
  resourceVersion: "56"
  selfLink: /apis/rbac.authorization.k8s.io/v1beta1/clusterrolebindingssystem%3Akube-dns
  uid: 9c79f3a2-80c9-11e7-aa28-005056a05d28
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-dns
subjects:
- kind: ServiceAccount
  name: kube-dns
  namespace: kube-system

#预定义的RoleBinding system:kube-dns将kube-system命名空间的kube-dns ServiceAccount与system:kube-dns Role绑定，该Role具有访问kube-apiserver DNS 相关API的权限
#kubedns-controller.yaml中定义的Pods时使用了kubedns-sa.yaml文件定义的 kube-dns ServiceAccount，所以具有访问kube-apiserver DNS相关API的权限


#配置 kube-dns ServiceAccount
#无需修改

#配置 kube-dns 服务
cd /root/cluster/dns
ls
kubedns-cm.yaml  kubedns-controller.yaml  kubedns-sa.yaml  kubedns-svc.yaml
#需要将kubedns-svc.yaml中的 spec.clusterIP 设置为集群环境变量中变量 CLUSTER_DNS_SVC_IP 值，这个 IP 需要和 kubelet 的 —cluster-dns 参数值一致
#kubedns-controller.yaml中的--domain 为集群环境文档 变量 CLUSTER_DNS_DOMAIN 的值；
#使用系统已经做了 RoleBinding 的 kube-dns ServiceAccount，该账户具有访问 kube-apiserver DNS 相关 API 的权限；

#执行所有文件
kubectl create -f .



#检查 kubedns 功能
cd /Data/app

#启动一个新service
cat > my-nginx.yaml <<EOF
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-nginx
spec:
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
EOF

kubectl create -f my-nginx.yaml

#Export 该 Deployment, 生成 my-nginx 服务
kubectl expose deploy my-nginx


#启动一个新pod
cat > pod-nginx.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.7.9
    ports:
    - containerPort: 80
EOF

kubectl create -f pod-nginx.yaml

#进入到新启动的pod中,列出所有pod，找到刚刚启动的新pod
kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                        READY     STATUS    RESTARTS   AGE       IP            NODE
default       my-nginx-3418754612-ntgr9   1/1       Running   0          24m       172.30.93.4   10.10.8.212
default       my-nginx-3418754612-trzmn   1/1       Running   0          24m       172.30.8.3    10.10.8.211
default       nginx                       1/1       Running   0          22m       172.30.8.4    10.10.8.211
default       nginx-ds-5ll7b              1/1       Running   1          1d        172.30.8.2    10.10.8.211
default       nginx-ds-7wnd1              1/1       Running   1          1d        172.30.93.2   10.10.8.212
kube-system   kube-dns-699984412-00wmp    3/3       Running   0          31m       172.30.93.3   10.10.8.212

kubectl exec -it my-nginx-3418754612-ntgr9 /bin/bash

root@my-nginx-3418754612-ntgr9:/#cat /etc/resolv.conf
nameserver 10.254.0.2
search default.svc.cluster.local. svc.cluster.local. cluster.local. localhost
options ndots:5

root@my-nginx-3418754612-ntgr9:/# ping my-nginx
PING my-nginx.default.svc.cluster.local (10.254.218.8): 48 data bytes
^C--- my-nginx.default.svc.cluster.local ping statistics ---
12 packets transmitted, 0 packets received, 100% packet loss
#可以将服务名解析成ip地址即为正常



################### 部署 dashboard 插件###########

mkdir /root/kubedns
cd /root/kubedns
ls
dashboard-controller.yaml  dashboard-rbac.yaml  dashboard-service.yaml
kubectl create -f .

#查看执行结果
kubectl get services kubernetes-dashboard -n kube-system
NAME                   CLUSTER-IP       EXTERNAL-IP   PORT(S)       AGE
kubernetes-dashboard   10.254.231.206   <nodes>       80:8888/TCP   8h

#检查 controller
kubectl get deployment kubernetes-dashboard  -n kube-system
NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-dashboard   1         1         1            1           8h

kubectl get pods  -n kube-system | grep dashboard
kubernetes-dashboard-3677875397-6cwwm   1/1       Running   1          8h

#访问dashboard
http://10.10.8.210:8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard




##########部署heapster插件##########
cd /Data/software
wget https://github.com/kubernetes/heapster/archive/v1.3.0.zip
unzip v1.3.0.zip
mv v1.3.0  heapster-1.3.0
cd heapster-1.3.0/deploy/kube-config/influxdb
ls
grafana-deployment.yaml  heapster-deployment.yaml  heapster-service.yaml  influxdb-deployment.yaml
grafana-service.yaml     heapster-rbac.yaml        influxdb-cm.yaml       influxdb-service.yaml

#执行所有定义文件
kubectl create -f .

#检查执行结果
kubectl get deployments -n kube-system | grep -E 'heapster|monitoring'
kubectl get pods -n kube-system | grep -E 'heapster|monitoring'


#访问grafana
http://10.10.8.210.7:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana

#访问influxdb admin UI
kubectl get svc -n kube-system|grep influxdb
monitoring-influxdb     10.254.126.38    <nodes>       8086:8992/TCP,8083:8425/TCP   <invalid>


http://10.10.8.210:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb:8083/
#在页面的 “Connection Settings” 的 Host 中输入 node IP， Port 中输入 8086映射的 nodePort 如上面的 8992，点击 “Save” 即可




########部署ELK插件#######
mkdir /root/elk
cd /root/elk
ls
es-controller.yaml es-rbac.yaml es-service.yaml  fluentd-es-ds.yaml  kibana-controller.yaml  kibana-service.yaml fluentd-es-rbac.yaml


#给node设置标签
#DaemonSet fluentd-es-v1.22 只会调度到设置了标签 beta.kubernetes.io/fluentd-ds-ready=true 的 Node，需要在期望运行 fluentd 的 Node 上设置该标签；
kubectl get nodes
10.10.8.211   Ready   1h        v1.6.2
10.10.8.212   Ready   1h        v1.6.2

kubectl label nodes 10.10.8.211 beta.kubernetes.io/fluentd-ds-ready=true
kubectl label nodes 10.10.8.212 beta.kubernetes.io/fluentd-ds-ready=true


#执行定义文件
cd /root/elk
kubectl create -f .

#检查执行结果
kubectl get deployment -n kube-system|grep kibana
kibana-logging         1         1         1            1           <invalid>



#kibana Pod 第一次启动时会用**较长时间(10-20分钟)**来优化和 Cache 状态页面，可以 tailf 该 Pod 的日志观察进度：
kubectl get pods -n kube-system|grep -E 'elasticsearch|fluentd|kibana'
elasticsearch-logging-v1-8gr1t          1/1       Running   0          <invalid>
elasticsearch-logging-v1-grcfs          1/1       Running   0          <invalid>
fluentd-es-v1.22-58z60                  1/1       Running   0          <invalid>
fluentd-es-v1.22-f7txp                  1/1       Running   0          <invalid>
kibana-logging-324921636-lpfw5          1/1       Running   0          <invalid>


kubectl logs kibana-logging-324921636-lpfw5 -n kube-system -f


#访问kibana
http://10.10.8.210:8080/api/v1/proxy/namespaces/kube-system/services/kibana-logging

#在 Settings -> Indices 页面创建一个 index（相当于 mysql 中的一个 database），选中 Index contains time-based events，使用默认的 logstash-* pattern，点击 Create ;
